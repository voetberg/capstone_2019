{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in(feat,label): \n",
    "    ##Simple read function\n",
    "    df_x = pd.read_csv(feat)\n",
    "    x = df_x.values\n",
    "    \n",
    "    df_y = pd.read_csv(label)\n",
    "    y = df_y.values\n",
    "    return x,y\n",
    "\n",
    "def mean_var(x): \n",
    "    '''Returns mean and variance of a set x'''\n",
    "    ##Calculate avg, use axis=0 to take it along the values, not the features\n",
    "    mu = np.mean(x,axis=0)\n",
    "    std = np.mean((x - mu)*(x - mu), axis=0)\n",
    "    return mu,std\n",
    "\n",
    "def normalize(train,test): \n",
    "    '''Function calling both testing and training sets, \n",
    "    must be normalized by the mean and variance\n",
    "    Normalize such that X = (X - mean)/sqrt(Var)'''\n",
    "    ##Call the mean_var function\n",
    "    mu,std = mean_var(train)\n",
    "    ##Normalize\n",
    "    train_norm = (train - mu)/(np.sqrt(std) +1e-15) ## 1e-15 added to avoid divide by 0 error\n",
    "    test_norm = (test - mu)/(np.sqrt(std) +1e-15)\n",
    "    return train_norm,test_norm\n",
    "\n",
    "def add_bias(X): \n",
    "    '''Adds a bias term to make sure we don't fuck up the model too bad'''\n",
    "    X_ones = np.ones(X.shape[0])\n",
    "    X_ones = X_ones.reshape(-1,1)\n",
    "    return np.append(X_ones,X,axis=1)\n",
    "\n",
    "def plot_raw_digit(feat): \n",
    "    '''Takes a feature vector and plots it as an 8x8 pixel image\n",
    "    Requires that the vector ranges from 0 to 16'''\n",
    "    plt.gray()\n",
    "    plt.matshow(feat.reshape(8,8))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = read_in('./data/Digits_0_1_X_train.csv',\"./data/Digits_0_1_y_train.csv\")\n",
    "x_test, y_test = read_in('./data/Digits_0_1_X_test.csv',\"./data/Digits_0_1_y_test.csv\")\n",
    "n_sample,n_feat = x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Normalize the features using Again. Just poached code from last class\n",
    "## This set doesn't Actually need to be normalized, but it's good practice \n",
    "x_train_norm,x_test_norm = normalize(x_train,x_test)\n",
    "x_train_norm_c0 = add_bias(x_train_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_raw_digit(x_train[105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result(x,c):\n",
    "    '''Uses a sigmoid function to predict\n",
    "    Requires that you calculate optimal coeffs\n",
    "    Returns array of predicted values'''\n",
    "    return 1/(1+np.exp(-x.dot(c)))\n",
    "\n",
    "def loss(x,c,y): \n",
    "    '''Loss function for a gradient descent predictor\n",
    "    This will be minimized using gradient descent\n",
    "    L(c) = 1/M*Sum(y*log(p_c)-(1-y)*log(1-p_c))\n",
    "    Designed to maximize when p_c = 1, minimize when p_c = 0\n",
    "    Returns real value'''\n",
    "    pred = result(x,c)\n",
    "    s = np.sum(-y.T.dot(np.log(pred))-((1-y).T).dot(np.log(1-pred)))\n",
    "    return float(1/x.shape[0]*s)\n",
    "\n",
    "def gradient_desc(X,y,l_rate=.01,ite=100,): \n",
    "    '''Input: \n",
    "    X-> Features (NxM), normalized w bias\n",
    "    y->Labels (Nx1)\n",
    "    learning rate (<<.01)\n",
    "    Iterations->Number of times to run \n",
    "    \n",
    "    Output: \n",
    "    c_n->Optimal coeffs\n",
    "    loss->loss value based off \n",
    "    '''\n",
    "    n_feat = X.shape[1] ##N value \n",
    "    c_n = np.zeros((n_feat,1)) ##initialize an empty matrix \n",
    "    loss_range = [0]*ite ##Empty matrix to document how loss changes with iteration\n",
    "    for i in range(ite): \n",
    "        ##Calculate the gradient over a range\n",
    "        grad = ????? ##Need to recalcuate this from how we did it in linear regress\n",
    "        c_n -=l_rate*grad\n",
    "        loss_range[i] = loss(X,c_n,y)\n",
    "        \n",
    "    return c_n, loss_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
