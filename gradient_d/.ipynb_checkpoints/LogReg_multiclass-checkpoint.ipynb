{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in(feat,label): \n",
    "    ##Simple read function\n",
    "    df_x = pd.read_csv(feat)\n",
    "    x = df_x.values\n",
    "    \n",
    "    df_y = pd.read_csv(label)\n",
    "    y = df_y.values\n",
    "    return x,y\n",
    "\n",
    "def mean_var(x): \n",
    "    '''Returns mean and variance of a set x'''\n",
    "    ##Calculate avg, use axis=0 to take it along the values, not the features\n",
    "    mu = np.mean(x,axis=0)\n",
    "    std = np.mean((x - mu)*(x - mu), axis=0)\n",
    "    return mu,std\n",
    "\n",
    "def normalize(train,test): \n",
    "    '''Function calling both testing and training sets, \n",
    "    must be normalized by the mean and variance\n",
    "    Normalize such that X = (X - mean)/sqrt(Var)'''\n",
    "    ##Call the mean_var function\n",
    "    mu,std = mean_var(train)\n",
    "    ##Normalize\n",
    "    train_norm = (train - mu)/(np.sqrt(std) +1e-15) ## 1e-15 added to avoid divide by 0 error\n",
    "    test_norm = (test - mu)/(np.sqrt(std) +1e-15)\n",
    "    return train_norm,test_norm\n",
    "\n",
    "def add_bias(X): \n",
    "    '''Adds a bias term to make sure we don't fuck up the model too bad'''\n",
    "    X_ones = np.ones(X.shape[0])\n",
    "    X_ones = X_ones.reshape(-1,1)\n",
    "    return np.append(X_ones,X,axis=1)\n",
    "\n",
    "def plot_raw_digit(feat): \n",
    "    '''Takes a feature vector and plots it as an 8x8 pixel image\n",
    "    Requires that the vector ranges from 0 to 16'''\n",
    "    plt.gray()\n",
    "    plt.matshow(feat.reshape(8,8))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = read_in('./data/Digits_X_train.csv',\"./data/Digits_y_train.csv\")\n",
    "x_test, y_test = read_in('./data/Digits_X_test.csv',\"./data/Digits_y_test.csv\")\n",
    "n_sample,n_feat = x_train.shape\n",
    "\n",
    "x_train_norm,x_test_norm = normalize(x_train,x_test)\n",
    "x_train_norm_c0,x_test_norm_c0 = add_bias(x_train_norm),add_bias(x_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_raw_digit(x_train[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are now different classes, so we have to deal with that. Our training is fucked. We need to make our labels binary for each round of training. Make it class a and not class a. This will be done with a one-hot-encoder. The ten labels of our sample becomes vectors such that: \n",
    "0->[1,0,0,0,...]\n",
    "\n",
    "1->[0,1,0,...]\n",
    "\n",
    "...\n",
    "\n",
    "9->[0,0,...,0,1]\n",
    "\n",
    "This functionally creates 10 different models, it will read down the columns and have a class of 1 and 0 for each column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##These are the same as a normal logistic regressor\n",
    "\n",
    "def result(x,c):\n",
    "    '''Uses a sigmoid function to predict\n",
    "    Requires that you calculate optimal coeffs\n",
    "    Returns array of predicted values'''\n",
    "    return 1/(1+np.exp(-x.dot(c)))\n",
    "\n",
    "def loss(x,c,y): \n",
    "    '''Loss function for a gradient descent predictor\n",
    "    This will be minimized using gradient descent\n",
    "    L(c) = 1/M*Sum(y*log(p_c)-(1-y)*log(1-p_c))\n",
    "    Designed to maximize when p_c = 1, minimize when p_c = 0\n",
    "    Returns real value'''\n",
    "    pred = result(x,c)\n",
    "    s = np.sum(-y.T.dot(np.log(pred))-((1-y.T)).dot(np.log(1-pred)))\n",
    "    return 1/x.shape[0]*s\n",
    "\n",
    "def gradient_desc(X,y,l_rate=.01,ite=100,): \n",
    "    '''Input: \n",
    "    X-> Features (NxM), normalized w bias\n",
    "    y->Labels (Nx1)\n",
    "    learning rate (<<.01)\n",
    "    Iterations->Number of times to run \n",
    "    \n",
    "    Output: \n",
    "    c_n->Optimal coeffs\n",
    "    loss->loss value based off \n",
    "    '''\n",
    "    n_feat = X.shape[1] ##N value \n",
    "    c_n = np.zeros((n_feat,1)) ##initialize an empty matrix \n",
    "    loss_range = [0]*ite ##Empty matrix to document how loss changes with iteration\n",
    "    for i in range(ite): \n",
    "        ##Calculate the gradient over a range\n",
    "        ##Gradient of the loss function, this is the simplified version \n",
    "        ## dL/dc = X^t*(y_pred-y_true)\n",
    "        x_t = X.T\n",
    "        grad = (1/X.shape[0])*x_t.dot(result(X,c_n)-y)\n",
    "        print(grad.shape)\n",
    "        c_n -=l_rate*grad\n",
    "        loss_range[i] = (loss(X,c_n,y)).ravel()\n",
    "        \n",
    "    return c_n, loss_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Binarize the labels \n",
    "\n",
    "def binary_labels(y): \n",
    "    '''One-hot-encoder vector labels \n",
    "    Uses sklearn preprocess package\n",
    "    Dont ask me how it works it just does'''\n",
    "    label = preprocessing.LabelBinarizer()\n",
    "    label.fit(y)\n",
    "    y_ohe = label.transform(y)\n",
    "    return y_ohe\n",
    "\n",
    "def multi_label_train(x,y): \n",
    "    '''Uses the ohe labels to train \n",
    "    Creates 10 different models, for each label\n",
    "    '''\n",
    "    c_lst = [] ##store for each model\n",
    "    for i in range(y.shape[1]): \n",
    "        y_one_col = y[:,i] ##Lines up the columns, trains for each one\n",
    "        c_one_col, loss_hist = gradient_desc(x,y_one_col,\n",
    "                                            ite = 200, \n",
    "                                            l_rate = 0.1)\n",
    "        c_lst.append(c_one_col)\n",
    "        \n",
    "    return(c_lst)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_ohe,y_test_ohe = binary_labels(y_train),binary_labels(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65, 1347)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (65,1) doesn't match the broadcast shape (65,1347)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-ce34a9e57d01>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mc_lst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmulti_label_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_norm_c0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train_ohe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-a0751dcf581b>\u001b[0m in \u001b[0;36mmulti_label_train\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m     19\u001b[0m         c_one_col, loss_hist = gradient_desc(x,y_one_col,\n\u001b[0;32m     20\u001b[0m                                             \u001b[0mite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m                                             l_rate = 0.1)\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mc_lst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_one_col\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-3199633c1751>\u001b[0m in \u001b[0;36mgradient_desc\u001b[1;34m(X, y, l_rate, ite)\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mc_n\u001b[0m \u001b[1;33m-=\u001b[0m\u001b[0ml_rate\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[0mloss_range\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc_n\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: non-broadcastable output operand with shape (65,1) doesn't match the broadcast shape (65,1347)"
     ]
    }
   ],
   "source": [
    "c_lst = multi_label_train(x_train_norm_c0,y_train_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Now we need to get the labels done for this \n",
    "##The sigmoid will return a probablity for each column\n",
    "def multi_pred(c_lst,x):\n",
    "    i=0\n",
    "    for c in c_lst: \n",
    "        prob = result(x,c)\n",
    "        if i==0: \n",
    "            prob_matrix = prob\n",
    "        else: \n",
    "            prob_matrix = np.concatenate((prob_matrix, prob),axis=1)\n",
    "        i+=1\n",
    "        label = [0,1,2,3,4,5,6,7,8,9]\n",
    "        \n",
    "        n_sample = x.shape[0]\n",
    "        y_pred = np.zeros(n_sample,dtype=int)\n",
    "        for i in range(n_sample): \n",
    "            y_pred[i] = label[np.argmax(prob_matrix[i,;])]\n",
    "            \n",
    "        return y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
