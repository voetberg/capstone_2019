{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in(feat,label): \n",
    "    ##Simple read function\n",
    "    df_x = pd.read_csv(feat)\n",
    "    x = df_x.values\n",
    "    \n",
    "    df_y = pd.read_csv(label)\n",
    "    y = df_y.values\n",
    "    return x,y\n",
    "\n",
    "def plot_raw_digit(feat): \n",
    "    '''Takes a feature vector and plots it as an 8x8 pixel image\n",
    "    Requires that the vector ranges from 0 to 16'''\n",
    "    plt.gray()\n",
    "    plt.matshow(feat.reshape(8,8))\n",
    "    plt.show()\n",
    "    \n",
    "def get_mean_variance(X):\n",
    "    mean = np.mean(X, axis=0) # axis=0: taking means along the\n",
    "    # vertical line (column)\n",
    "    # (sum(x_i-\\mu)^2)/N\n",
    "    X_temp = X - mean #\n",
    "    X_temp_entrypointwise = X_temp*X_temp\n",
    "    variance = np.mean(X_temp_entrypointwise, axis=0) #axis=0: \n",
    "    # taking means along the vertical line (column)\n",
    "    return mean, variance\n",
    "    \n",
    "def normalize_features(X_train, X_test):\n",
    "    mean, variance = get_mean_variance(X_train)\n",
    "    variance += 1e-15\n",
    "    ''' transform the feature '''\n",
    "    X_train_norm = (X_train - mean)/np.sqrt(variance)\n",
    "    #math.sqrt doesnot work for numpy\n",
    "    X_test_norm = (X_test - mean)/np.sqrt(variance)\n",
    "    return X_train_norm, X_test_norm\n",
    "\n",
    "def add_column_one(X):\n",
    "    '''\n",
    "         convert X -> [1 X]\n",
    "    '''\n",
    "    # add  column of ones\n",
    "    ones = np.ones(X.shape[0])\n",
    "    ones = ones.reshape(-1, 1)\n",
    "    X_new = np.append(ones, X, axis=1)\n",
    "\n",
    "    return X_new    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pwd = '/Users/offic/repositories/ml_capstone/gradient_d/data/'\n",
    "x_train, y_train = read_in(pwd+'Digits_X_train.csv', pwd+'Digits_y_train.csv')\n",
    "x_test, y_test = read_in(pwd+'Digits_X_test.csv', pwd+'Digits_y_test.csv')\n",
    "\n",
    "x_train_norm, x_test_norm = normalize_features(x_train,x_test)\n",
    "x_train_norm_bias, x_test_norm_bias = add_column_one(x_train_norm),add_column_one(x_test_norm)\n",
    "\n",
    "plot_raw_digit(x_train[0]) ##Yep we got the numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(c,x): \n",
    "    '''Just a linear combo between coeffients and features'''\n",
    "    return X.dot(c)\n",
    "\n",
    "def loss(y_true,y_pred,c, lamb):\n",
    "    '''Function to be minimzied\n",
    "    L = norm(c) + K*sum(max(0, 1-y*cT*x))\n",
    "    '''\n",
    "    sqC = np.sum(c*c) - c[0]*c[0] ##Remove the bias term in the loss\n",
    "    hinge = 1-y_true*y_pred\n",
    "    hinge[hinge<0] = 0 ##eq to a if statement \n",
    "    return sqC + lamb*np.sum(hinge)\n",
    "\n",
    "## THEN GRADIENT DECENT. Just need to add a lambda parameter \n",
    "\n",
    "def gradient_desc(x,y,l_rate=.01,ite=100, lamb): \n",
    "    '''Input: \n",
    "    X-> Features (NxM), normalized w bias\n",
    "    y->Labels (Nx1)\n",
    "    learning rate (<<.01)\n",
    "    Iterations->Number of times to run \n",
    "    \n",
    "    Output: \n",
    "    c_n->Optimal coeffs\n",
    "    loss->loss value based off \n",
    "    '''\n",
    "    n_feat = X.shape[1] ##N value \n",
    "    c_n = np.zeros((n_feat,1)) ##initialize an empty matrix \n",
    "    loss_range = [0]*ite ##Empty matrix to document how loss changes with iteration\n",
    "    for i in range(ite): \n",
    "        ##Calculate the gradient over a range\n",
    "        ##Gradient of the loss function, this is the simplified version \n",
    "        ## dL/dc = X^t*(y_pred-y_true)\n",
    "        y_pred = predictor(c_n,x)\n",
    "        loss_range[i] = loss(y_pred, y,c,lamb).ravel[0] \n",
    "        \n",
    "        grad = np.zeros((n_feat,1))\n",
    "        grad[1:]+=2*c_n[1:]\n",
    "        hinge = 1 - y*y_pred\n",
    "        for i in range(hinge.size):\n",
    "            if(hinge[i]>0):\n",
    "                gradient+= (-y[i]*x[i,:].T).reshape\n",
    "        \n",
    "    return c_n, loss_range"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
